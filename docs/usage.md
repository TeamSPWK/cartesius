# Usage

After installing the package, you can use the command **`cartesius`** to train and test a model.

## How to train ?

By default the `cartesius` command uses the default configuration, located in `cartesius/config/default.yaml` :

```bash
cartesius
```

Once the training started, it will log all information in `wandb`.

Training results (including checkpoints) are saved in `results` folder by default.

---

You can change the configuration by specifying a different configuration file :

```bash
cartesius config=transformer.yaml
```

!!! hint
    You can directly specify the name of the configuration file if the file is located in `cartesius/config`, or you can specify a regular path to your configuration file. 

---

You can also change each value of the configuration independently, directly from the command line :

```bash
cartesius seed=666 activation=relu
```

## How to test ?

By default, the `cartesius` will **automatically** run tests **after training**, using the best checkpoint.

---

You can test a specific checkpoint by specifying its path :

```bash
cartesius train=False test=True ckpt=<path/to/my/model.ckpt>
```

!!! tip
    After testing, a file `submission.csv` will be saved. You can submit this file in the [Kaggle competition](https://www.kaggle.com/c/cartesius/).

## Data

### Train data

The train dataset is generated **randomly**.

Each sample of the dataset is a **randomly generated polygon**, with an **arbitrary position**, **arbitrary scale**, etc...

For each polygon, labels are generated, and the model is trained to predict these labels.

Each task has a different label.

### Validation & Test data

The test dataset is a list of **handcrafted** polygons.

Similarly to the train dataset, labels are generated for each polygon and the models are evaluated on their predictions of those labels.

!!! check "Info"
    The test set is trying to cover as many different cases of polygons as possible, including types of polygon that can't be generated by the train dataset, in order to assess the generalization capabilities of the models.

!!! important
    It's very possible that the test set is not complete. It's still a work in progress, feel free to propose some changes !

The validation dataset is exactly like the test dataset, but with different (arbitrary) position, orientation, scale, etc...

!!! danger "Advanced"
    You can find the script used to generate the polygons in the [Generating handcrafted polygons for test/validation](gen_handcraft_poly.md) page.

## Tasks

Models are trained and evaluated on several tasks. The goal is to get good representation of the input polygon, good enough to properly predict the labels of all tasks.

List of tasks currently implemented :

* **`area`** : Predict the area of the polygon.
* **`perimeter`** : Predict the perimeter of the polygon.
* **`size`** : Predict the width and the height of the polygon.
* **`convexity`** : Predict the "convexity" of the polygon. Convexity is defined as the area of the polygon divided by the area of its convex hull. It represents "how much convex a polygon is".
* **`min_clear`** : Predict the minimum clearance of the polygon. The minimum clearance is the smallest distance by which a node should be moved to produce an invalid geometry.
* **`centroid`** : Predict the x and y coordinates of the centroid of the polygon.
* **`ombr_ratio`** : Predict "oriented minimum bounding rectangle ratio" of the polygon. OMBR ratio is defined as the area of the polygon divided by the area of its oriented minimum bounding rectangle. It represents "how close the polygon is to a rectangle".
* **`aspect_ratio`** : Predict "oriented minimum bounding rectangle aspect ratio" of the polygon. OMBR aspect ratio is defined as the length of shorter line divided by longer line of the OMBR. It represents wideness of the polygon.
* **`opening_ratio`** : Predict "opening ratio" of the polygon. Opening ratio is defined as the area of the opening applied polygon divided by the area of the original polygon. It represents "how much polygon have useful space that is wide enough".

## Configuration

### Metadata

**`parent_config`** : Name of the configuration file to use as a parent (this configuration will inherit the values from the parent as default).

**`description`** : Description of this configuration.

### General

**`project_name`** : Name of the project in `wandb`.

**`seed`** : Seed to use. If `None`, a random seed is used.

**`train`** : Whether to run training or not.

**`test`** : Whether to run testing or not.

### Paths

**`save_dir`** : Name of the folder where to save the results.

**`ckpt`** : Path of the checkpoint to load.

**`val_set_file`** : Path to the JSON file containing the validation set.

**`test_set_file`** : Path to the JSON file containing the test set.

**`kaggle_submission_file`** : Path where to save the CSV file that can be used as submission file for the Kaggle competition.

### Model

**`model_name`** : Name of the model to use.

**`d_model`** : Dimension of the model.

**`max_seq_len`** : Maximum sequence length.

**`n_heads`** : Number of attention heads to use in Transformer.

**`d_ff`** : Dimension for the hidden size of the FF network in Transformer.

**`dropout`** : Dropout ratio.

**`activation`** : Activation function to use for Transformer. Can be `gelu` or `relu`.

**`n_layers`** : Number of Transformer layers.

**`pooling`** : Type of pooling for representing the whole polygon in Transformer. Can be `first`, `mean`, or `max`.

**`adjacent_only`** : If set to `True`, an adjency matrix will be used in SE(3)-Transformer. Otherwise all nodes are attended.

### Tasks

**`tasks`** : Names of the tasks to use.

**`tasks_scales`** : Scales for the tasks' losses.

**`task_dropout`** : Dropout ratio for the task regression heads.

### Training

**`batch_size`** : Batch size.

**`n_batch_per_epoch`** : Number of batch to consider in one epoch.

**`max_time`** : Maximum time alloted to training.

**`n_workers`** : Number of loader worker processes (DataLoader).

**`watch_model`** : Whether to watch model or not with `wandb`.

### Optimizer & LR Scheduler

**`lr`** : Learning rate.

**`auto_lr_find`** : Whether to find LR automatically or not.

**`grad_clip`** : Gradient clipping.

**`scheduler`**: Name of the LR scheduler to user. Can be `cosannwarm` or null.

**`sched_T_0`**: When using CosineAnnealingWarmRestart scheduler, this is the number of iterations for the first restart.

**`sched_T_mult`**: When using CosineAnnealingWarmRestart scheduler, this is the factor increases after a restart.

**`sched_min_lr_ratio`**: When using CosineAnnealingWarmRestart scheduler, this is the minimum learning rate.

### Polygon generation

**`x_range`** : Allowed range for the polygon center (x-axis).

**`y_range`** : Allowed range for the polygon center (y-axis).

**`n_range`** : Possible choices for the number of points generated in the polygon.

**`avg_radius_range`** : Possible choices for the average radius of the generated polygon.

### Data handling
 
**`transforms`** : Tranforms' names to apply on the data.

**`tokenizer`** : Name of the tokenizer to use.
